{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8d3a9c-7c89-4de4-ae93-1c4b5f0a7b9f",
   "metadata": {},
   "source": [
    "# UFC Fight Outcome Prediction - Machine Learning Models\n",
    "\n",
    "This notebook implements machine learning models to predict UFC fight outcomes based on fighter statistics and historical performance data.\n",
    "\n",
    "## Objectives:\n",
    "1. Predict fight outcomes (Red vs Blue corner winner)\n",
    "2. Identify the most important features for predicting fight outcomes\n",
    "3. Compare multiple machine learning algorithms\n",
    "4. Evaluate model performance using appropriate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries with error handling\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for required libraries\n",
    "required_libs = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'sklearn']\n",
    "missing_libs = []\n",
    "\n",
    "for lib in required_libs:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "        print(f\"‚úì {lib} is available\")\n",
    "    except ImportError:\n",
    "        missing_libs.append(lib)\n",
    "        print(f\"‚úó {lib} is missing\")\n",
    "\n",
    "if missing_libs:\n",
    "    print(f\"\\n‚ùå Missing required libraries: {missing_libs}\")\n",
    "    print(\"Please install them using: pip install\", ' '.join(missing_libs))\n",
    "    print(\"\\nNote: This notebook requires the following dependencies:\")\n",
    "    print(\"- pandas: for data manipulation\")\n",
    "    print(\"- numpy: for numerical computations\")\n",
    "    print(\"- matplotlib & seaborn: for data visualization\")\n",
    "    print(\"- scikit-learn: for machine learning models\")\n",
    "    sys.exit(\"Stopping execution due to missing dependencies.\")\n",
    "else:\n",
    "    # Import all required libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"\\nüéâ All libraries imported successfully!\")\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "    print(f\"Pandas version: {pd.__version__}\")\n",
    "    print(f\"NumPy version: {np.__version__}\")\n",
    "    print(f\"Scikit-learn version: {__import__('sklearn').__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c4d5e6-7891-4abc-def0-123456789abc",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234abcd-5678-90ef-1234-567890abcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UFC dataset with enhanced error handling\n",
    "import os\n",
    "\n",
    "# Check if required libraries are available\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing required library: {e}\")\n",
    "    print(\"Please install required libraries: pip install pandas numpy scikit-learn matplotlib seaborn\")\n",
    "    df = None\n",
    "else:\n",
    "    # Try different data file paths\n",
    "    data_paths = [\n",
    "        '../data/data.csv',\n",
    "        './data/data.csv',\n",
    "        'data/data.csv'\n",
    "    ]\n",
    "    \n",
    "    df = None\n",
    "    for path in data_paths:\n",
    "        try:\n",
    "            if os.path.exists(path):\n",
    "                df = pd.read_csv(path)\n",
    "                print(f\"‚úÖ Data loaded successfully from: {path}\")\n",
    "                print(f\"üìä Dataset shape: {df.shape}\")\n",
    "                print(f\"üìã Columns available: {len(df.columns)}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load data from {path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"\\n‚ùå Could not find or load UFC data file.\")\n",
    "        print(\"Expected data file locations:\")\n",
    "        for path in data_paths:\n",
    "            exists = \"‚úì\" if os.path.exists(path) else \"‚úó\"\n",
    "            print(f\"  {exists} {path}\")\n",
    "        \n",
    "        print(\"\\nüìù This notebook expects a CSV file with UFC fight data containing:\")\n",
    "        print(\"  - Fighter statistics (R_ and B_ prefixed columns)\")\n",
    "        print(\"  - Winner column ('Red' or 'Blue')\")\n",
    "        print(\"  - Fight metadata (date, location, weight_class, etc.)\")\n",
    "    else:\n",
    "        # Show first few columns to verify data structure\n",
    "        print(f\"\\nüìã First 10 columns: {df.columns.tolist()[:10]}\")\n",
    "        \n",
    "        # Fix column name issues (remove special characters)\n",
    "        df.columns = df.columns.str.replace('‚â§', '', regex=False)\n",
    "        df.columns = df.columns.str.replace('Ôªø', '', regex=False)  # Remove BOM if present\n",
    "        df.columns = df.columns.str.strip()  # Remove leading/trailing whitespace\n",
    "        \n",
    "        print(f\"üìã Cleaned columns (first 10): {df.columns.tolist()[:10]}\")\n",
    "        \n",
    "        if 'Winner' in df.columns:\n",
    "            print(f\"üéØ Winner column found: {df['Winner'].value_counts().to_dict()}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Warning: 'Winner' column not found in dataset\")\n",
    "            print(f\"Available columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345bcde-6789-01ef-2345-678901bcdefg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "if df is not None:\n",
    "    print(\"=== Dataset Overview ===\")\n",
    "    print(f\"Total fights: {len(df)}\")\n",
    "    print(f\"Total features: {len(df.columns)}\")\n",
    "    print(f\"\\nWinner distribution:\")\n",
    "    print(df['Winner'].value_counts())\n",
    "    print(f\"\\nWeight classes:\")\n",
    "    print(df['weight_class'].value_counts())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "    print(f\"\\nTop 10 columns with missing values:\")\n",
    "    print(missing_pct.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-7891-4abc-def0-123456789abc",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456cdef-7890-12ef-3456-789012cdefgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the UFC dataset for machine learning.\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Create target variable (1 if Red wins, 0 if Blue wins)\n",
    "    df_processed['red_wins'] = (df_processed['Winner'] == 'Red').astype(int)\n",
    "    \n",
    "    # Select numeric features only (excluding target and non-predictive columns)\n",
    "    exclude_cols = ['R_fighter', 'B_fighter', 'Referee', 'date', 'location', \n",
    "                   'Winner', 'red_wins', 'title_bout']\n",
    "    \n",
    "    # Get numeric columns\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove excluded columns from numeric columns\n",
    "    feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"Selected {len(feature_cols)} numeric features for modeling\")\n",
    "    \n",
    "    # Handle categorical variables if needed\n",
    "    categorical_cols = ['R_Stance', 'B_Stance', 'weight_class']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df_processed.columns:\n",
    "            # Create dummy variables\n",
    "            dummies = pd.get_dummies(df_processed[col], prefix=col, drop_first=True)\n",
    "            df_processed = pd.concat([df_processed, dummies], axis=1)\n",
    "            feature_cols.extend(dummies.columns.tolist())\n",
    "    \n",
    "    # Return processed dataframe and feature columns\n",
    "    return df_processed, feature_cols\n",
    "\n",
    "# Process the data\n",
    "if df is not None:\n",
    "    df_processed, feature_columns = preprocess_data(df)\n",
    "    print(f\"\\nProcessed dataset shape: {df_processed.shape}\")\n",
    "    print(f\"Number of features for modeling: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567def0-8901-23ef-4567-890123def012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "if df is not None:\n",
    "    # Extract features and target\n",
    "    X = df_processed[feature_columns]\n",
    "    y = df_processed['red_wins']\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
    "    \n",
    "    print(f\"\\nMissing values after imputation: {X_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f678-9012-5bcd-ef01-234567890123",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split and Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678ef01-9012-34ef-5678-901234ef0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "if df is not None:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_imputed, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    print(f\"Training target distribution: {y_train.value_counts().to_dict()}\")\n",
    "    print(f\"Test target distribution: {y_test.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"\\nFeatures scaled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f67890-0123-6cde-f012-345678901234",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789f012-0123-45ef-6789-012345f01234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "if df is not None:\n",
    "    print(\"Training models...\\n\")\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        # Use scaled data for all models\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc_score,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  AUC: {auc_score:.4f}\")\n",
    "        print(f\"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        print()\n",
    "    \n",
    "    print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6789012-1234-7def-0123-456789012345",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789012f3-1234-56ef-7890-123456f01234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary\n",
    "if df is not None and results:\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'Test Accuracy': [results[name]['accuracy'] for name in results.keys()],\n",
    "        'AUC Score': [results[name]['auc'] for name in results.keys()],\n",
    "        'CV Mean': [results[name]['cv_mean'] for name in results.keys()],\n",
    "        'CV Std': [results[name]['cv_std'] for name in results.keys()]\n",
    "    })\n",
    "    \n",
    "    results_df = results_df.sort_values('Test Accuracy', ascending=False)\n",
    "    print(\"=== Model Performance Summary ===\")\n",
    "    print(results_df.round(4))\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = results_df.iloc[0]['Model']\n",
    "    best_model = results[best_model_name]['model']\n",
    "    print(f\"\\nBest performing model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890123f4-2345-67ef-8901-234567f01234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "if df is not None and results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Accuracy comparison\n",
    "    axes[0, 0].bar(results_df['Model'], results_df['Test Accuracy'])\n",
    "    axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. AUC comparison\n",
    "    axes[0, 1].bar(results_df['Model'], results_df['AUC Score'])\n",
    "    axes[0, 1].set_title('Model AUC Comparison')\n",
    "    axes[0, 1].set_ylabel('AUC Score')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. ROC Curves\n",
    "    for name in results.keys():\n",
    "        fpr, tpr, _ = roc_curve(y_test, results[name]['probabilities'])\n",
    "        axes[1, 0].plot(fpr, tpr, label=f\"{name} (AUC = {results[name]['auc']:.3f})\")\n",
    "    \n",
    "    axes[1, 0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    axes[1, 0].set_xlabel('False Positive Rate')\n",
    "    axes[1, 0].set_ylabel('True Positive Rate')\n",
    "    axes[1, 0].set_title('ROC Curves')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Confusion Matrix for best model\n",
    "    cm = confusion_matrix(y_test, results[best_model_name]['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=axes[1, 1], \n",
    "                xticklabels=['Blue Wins', 'Red Wins'],\n",
    "                yticklabels=['Blue Wins', 'Red Wins'])\n",
    "    axes[1, 1].set_title(f'Confusion Matrix - {best_model_name}')\n",
    "    axes[1, 1].set_ylabel('True Label')\n",
    "    axes[1, 1].set_xlabel('Predicted Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901234f5-3456-78ef-9012-345678f01234",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012345f6-4567-89ef-0123-456789f01234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "if df is not None and results:\n",
    "    tree_models = ['Random Forest', 'Gradient Boosting']\n",
    "    \n",
    "    for model_name in tree_models:\n",
    "        if model_name in results:\n",
    "            model = results[model_name]['model']\n",
    "            \n",
    "            # Get feature importance\n",
    "            importance = model.feature_importances_\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': feature_columns,\n",
    "                'importance': importance\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Plot top 20 features\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            top_features = feature_importance.head(20)\n",
    "            plt.barh(range(len(top_features)), top_features['importance'])\n",
    "            plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title(f'Top 20 Feature Importance - {model_name}')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\n=== Top 10 Most Important Features ({model_name}) ===\")\n",
    "            print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123456f7-5678-90ef-1234-567890f01234",
   "metadata": {},
   "source": [
    "## 7. Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234567f8-6789-01ef-2345-678901f01234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for the best model\n",
    "if df is not None and results:\n",
    "    print(f\"Performing hyperparameter tuning for {best_model_name}...\")\n",
    "    \n",
    "    if best_model_name == 'Random Forest':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "        base_model = RandomForestClassifier(random_state=42)\n",
    "        \n",
    "    elif best_model_name == 'Gradient Boosting':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "        base_model = GradientBoostingClassifier(random_state=42)\n",
    "        \n",
    "    elif best_model_name == 'Logistic Regression':\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear']\n",
    "        }\n",
    "        base_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        \n",
    "    else:  # SVM\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "        base_model = SVC(random_state=42, probability=True)\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        base_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Best parameters and score\n",
    "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Test the tuned model\n",
    "    tuned_model = grid_search.best_estimator_\n",
    "    y_pred_tuned = tuned_model.predict(X_test_scaled)\n",
    "    y_pred_proba_tuned = tuned_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "    tuned_auc = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "    \n",
    "    print(f\"\\nTuned model performance:\")\n",
    "    print(f\"  Accuracy: {tuned_accuracy:.4f}\")\n",
    "    print(f\"  AUC: {tuned_auc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nImprovement over default:\")\n",
    "    print(f\"  Accuracy: {tuned_accuracy - results[best_model_name]['accuracy']:.4f}\")\n",
    "    print(f\"  AUC: {tuned_auc - results[best_model_name]['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345678f9-7890-12ef-3456-789012f01234",
   "metadata": {},
   "source": [
    "## 8. Model Interpretation and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456789fa-8901-23ef-4567-890123f01234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model evaluation and classification report\n",
    "if df is not None and results:\n",
    "    print(\"=== Detailed Classification Report (Best Model) ===\")\n",
    "    print(classification_report(y_test, results[best_model_name]['predictions'],\n",
    "                              target_names=['Blue Wins', 'Red Wins']))\n",
    "    \n",
    "    # Model predictions distribution\n",
    "    pred_proba = results[best_model_name]['probabilities']\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Subplot 1: Prediction probability distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(pred_proba, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Predicted Probability (Red Wins)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Prediction Probabilities')\n",
    "    plt.axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Subplot 2: Calibration plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    \n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_test, pred_proba, n_bins=10\n",
    "    )\n",
    "    \n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=best_model_name)\n",
    "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title('Calibration Plot')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56789012-9012-34ef-5678-901234567890",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67890123-0123-45ef-6789-012345678901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "if df is not None and results:\n",
    "    print(\"=== UFC Fight Outcome Prediction - Summary ===\")\n",
    "    print(f\"\\nüìä Dataset: {len(df)} fights with {len(feature_columns)} features\")\n",
    "    print(f\"üéØ Target: Predict Red corner wins (vs Blue corner wins)\")\n",
    "    print(f\"üìà Best Model: {best_model_name}\")\n",
    "    print(f\"üéØ Best Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "    print(f\"üìä Best AUC Score: {results[best_model_name]['auc']:.4f}\")\n",
    "    \n",
    "    if 'tuned_accuracy' in locals():\n",
    "        print(f\"‚ö° Tuned Accuracy: {tuned_accuracy:.4f}\")\n",
    "        print(f\"‚ö° Tuned AUC Score: {tuned_auc:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Key Insights ===\")\n",
    "    print(\"‚Ä¢ Fight outcomes can be predicted with reasonable accuracy using fighter statistics\")\n",
    "    print(\"‚Ä¢ Historical performance metrics are strong predictors of fight outcomes\")\n",
    "    print(\"‚Ä¢ Tree-based models (Random Forest, Gradient Boosting) tend to perform well\")\n",
    "    print(\"‚Ä¢ Feature engineering and hyperparameter tuning can improve model performance\")\n",
    "    \n",
    "    print(\"\\n=== Next Steps ===\")\n",
    "    print(\"‚Ä¢ Collect more recent fight data to improve model accuracy\")\n",
    "    print(\"‚Ä¢ Engineer additional features (momentum, recent form, style matchups)\")\n",
    "    print(\"‚Ä¢ Implement ensemble methods combining multiple models\")\n",
    "    print(\"‚Ä¢ Deploy the model for real-time fight outcome predictions\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Error: Unable to complete analysis due to data loading issues.\")\n",
    "    print(\"Please ensure the data file exists at '../data/data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78901234-1234-56ef-7890-123456789012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model (optional)\n",
    "if df is not None and results:\n",
    "    import joblib\n",
    "    \n",
    "    try:\n",
    "        # Save the best model and scaler\n",
    "        joblib.dump(best_model, f'../models/best_ufc_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl')\n",
    "        joblib.dump(scaler, '../models/feature_scaler.pkl')\n",
    "        \n",
    "        print(f\"‚úÖ Best model ({best_model_name}) saved successfully!\")\n",
    "        print(\"üìÅ Model file: ../models/best_ufc_model_*.pkl\")\n",
    "        print(\"üìÅ Scaler file: ../models/feature_scaler.pkl\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not save model: {e}\")\n",
    "        print(\"This is normal if the models directory doesn't exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}